{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Notebook\n",
    "- Evaluating perplexity and hallucination (Automated evaluation metrics)\n",
    "- Perplexity is calculated using Huggingface\n",
    "- Hallucination is calculated using SelfCheckGPT score/s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Installation / Google Drive Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install selfcheckgpt\n",
    "! pip install spacy\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loading LORA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading LORA Model\n",
    "if True:\n",
    "    max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "    dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "    load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "    from unsloth import FastLanguageModel\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = \"/content/drive/MyDrive/Fine-Tuning Llama 3.1 For Test Cases/lora\", # YOUR MODEL YOU USED FOR TRAINING\n",
    "        max_seq_length = max_seq_length,\n",
    "        dtype = dtype,\n",
    "        load_in_4bit = load_in_4bit,\n",
    "    )\n",
    "    FastLanguageModel.for_inference(model) # Enable native 2x faster inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(\"/content/drive/MyDrive/Fine-Tuning Llama 3.1 For Test Cases/cycle2_test_reformatted.csv\")\n",
    "\n",
    "# Combine the columns into a single text input for the model\n",
    "df[\"text\"] = f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{df[\"instruction\"]}\n",
    "\n",
    "### Input:\n",
    "{df[\"input\"]}\n",
    "\n",
    "### Response:\n",
    "{df[\"output\"]}\"\"\"\n",
    "\n",
    "# Create a HuggingFace dataset\n",
    "dataset = Dataset.from_pandas(df[[\"text\"]])\n",
    "\n",
    "# Preprocess the dataset to match the input format\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(preprocess_function, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "# Evaluate Perplexity using Torch\n",
    "def compute_perplexity(model, dataset, tokenizer):\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "    dataloader = DataLoader(tokenized_datasets, collate_fn=data_collator, batch_size=8)\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_length = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader):\n",
    "            inputs = batch[\"input_ids\"].to(device)\n",
    "            outputs = model(inputs, labels=inputs)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item() * inputs.size(1)\n",
    "            total_length += inputs.size(1)\n",
    "\n",
    "    avg_loss = total_loss / total_length\n",
    "    perplexity = torch.exp(torch.tensor(avg_loss))\n",
    "    return perplexity.item()\n",
    "\n",
    "perplexity = compute_perplexity(model, dataset, tokenizer)\n",
    "print(f\"Perplexity: {perplexity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hallucination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy\n",
    "!pip install selfcheckgpt\n",
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import spacy\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "from selfcheckgpt.modeling_selfcheck import SelfCheckLLMPrompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_model = model\n",
    "main_tokenizer = tokenizer\n",
    "\n",
    "torch.manual_seed(28)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)main_\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device_map=\"auto\")\n",
    "\n",
    "prompt = \"\"\"\n",
    "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "Generate a test case for the following UI Element: Link Element 'Euskara' With URL https://eu.wikipedia.org/wiki/ from the website: https://en.wikipedia.org/wiki/Main_Page\n",
    "\n",
    "### Input:\n",
    "\n",
    "\n",
    "### Response:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# As per the original paper the response is generated with greedy decoding\n",
    "Response = pipe(prompt, do_sample=False, max_new_tokens=128, return_full_text=False)\n",
    "Response\n",
    "\n",
    "# The samples are generated for the same prompt with temperature as 1.\n",
    "N = 20\n",
    "Samples = pipe(\n",
    "    [prompt] * N,\n",
    "    temperature=1.0,\n",
    "    do_sample=True,\n",
    "    max_new_tokens=128,\n",
    "    return_full_text=False,\n",
    ")\n",
    "print(Samples[0])\n",
    "\n",
    "Response = Response[0][\"generated_text\"]\n",
    "Samples = [sample[0][\"generated_text\"] for sample in Samples]\n",
    "\n",
    "from typing import Dict, List, Set, Tuple, Union\n",
    "\n",
    "main_model.eval()\n",
    "if device is None:\n",
    "    device = torch.device(\"cpu\")\n",
    "main_model.to(device)\n",
    "device = device\n",
    "prompt_template = \"Context: {context}\\n\\nSentence: {sentence}\\n\\nIs the sentence supported by the context above? Answer Yes or No.\\n\\nAnswer: \"\n",
    "text_mapping = {'yes': 0.0, 'no': 1.0, 'n/a': 0.5}\n",
    "not_defined_text = set()\n",
    "print(f\"SelfCheck-LLMPrompt ({main_model}) initialized to device {device}\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "sentences = [\n",
    "    sent.text.strip() for sent in nlp(Response).sents\n",
    "]  # spacy sentence tokenization\n",
    "# print(sentences)\n",
    "\n",
    "def pred(sentences, sampled_passages, verbose=True):\n",
    "  num_sentences = len(sentences)\n",
    "  num_samples = len(sampled_passages)\n",
    "  scores = np.zeros((num_sentences, num_samples))\n",
    "  disable = not verbose\n",
    "  for sent_i in tqdm(range(num_sentences), disable=disable):\n",
    "      sentence = sentences[sent_i]\n",
    "      for sample_i, sample in enumerate(sampled_passages):\n",
    "\n",
    "          # this seems to improve performance when using the simple prompt template\n",
    "          sample = sample.replace(\"\\n\", \" \")\n",
    "\n",
    "          prompt = \"Context: {context}\\n\\nSentence: {sentence}\\n\\nIs the sentence supported by the context above? Answer Yes or No.\\n\\nAnswer: \".format(context=sample, sentence=sentence)\n",
    "          inputs = main_tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "          generate_ids = main_model.generate(\n",
    "              inputs.input_ids,\n",
    "              max_new_tokens=5,\n",
    "              do_sample=False, # hf's default for Llama2 is True\n",
    "          )\n",
    "          output_text = main_tokenizer.batch_decode(\n",
    "              generate_ids, skip_special_tokens=True,\n",
    "              clean_up_tokenization_spaces=False\n",
    "          )[0]\n",
    "          generate_text = output_text.replace(prompt, \"\")\n",
    "          score_ = text_postprocessing(generate_text)\n",
    "          scores[sent_i, sample_i] = score_\n",
    "  scores_per_sentence = scores.mean(axis=-1)\n",
    "  return scores_per_sentence\n",
    "\n",
    "def text_postprocessing(text):\n",
    "  \"\"\"\n",
    "  To map from generated text to score\n",
    "  Yes -> 0.0\n",
    "  No  -> 1.0\n",
    "  everything else -> 0.5\n",
    "  \"\"\"\n",
    "  # tested on Llama-2-chat (7B, 13B) --- this code has 100% coverage on wikibio gpt3 generated data\n",
    "  # however it may not work with other datasets, or LLMs\n",
    "  text = text.lower().strip()\n",
    "  if text[:3] == 'yes':\n",
    "      text = 'yes'\n",
    "  elif text[:2] == 'no':\n",
    "      text = 'no'\n",
    "  else:\n",
    "      if text not in not_defined_text:\n",
    "          # print(f\"warning: {text} not defined\")\n",
    "          not_defined_text.add(text)\n",
    "      text = 'n/a'\n",
    "  return text_mapping[text]\n",
    "\n",
    "sent_scores_prompt = pred(\n",
    "    sentences=sentences,  # list of sentences\n",
    "    sampled_passages=Samples,  # list of sampled passages\n",
    "    verbose=True,  # whether to show a progress bar\n",
    ")\n",
    "\n",
    "print(sent_scores_prompt)\n",
    "print(\"Hallucination Score:\", np.mean(sent_scores_prompt))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
