{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Notebook\n",
    "- Evaluating perplexity and hallucination (Automated evaluation metrics)\n",
    "- Perplexity is calculated using Huggingface\n",
    "- Hallucination is calculated using SelfCheckGPT score/s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Installation / Google Drive Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install selfcheckgpt\n",
    "! pip install spacy\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Evaluate Perplexity using Torch\n",
    "def compute_perplexity(model, dataset, tokenizer):\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "    dataloader = DataLoader(dataset, collate_fn=data_collator, batch_size=8)\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_length = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader):\n",
    "            inputs = batch[\"input_ids\"]\n",
    "            outputs = model(inputs, labels=inputs)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item() * inputs.size(1)\n",
    "            total_length += inputs.size(1)\n",
    "    \n",
    "    perplexity = torch.exp(total_loss / total_length)\n",
    "    return perplexity.item()\n",
    "\n",
    "# Load Model\n",
    "model_location = \"\"\n",
    "# Directory of pytorch .bin files\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_location)\n",
    "model = LlamaForCausalLM.from_pretrained(model_location)\n",
    "\n",
    "# Perplexity Evaluation Dataset\n",
    "dataset_location = \"\"\n",
    "eval_dataset = load_dataset(dataset_location)\n",
    "\n",
    "perplexity = compute_perplexity(model, eval_dataset, tokenizer)\n",
    "print(f\"Perplexity: {perplexity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hallucination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Call Model To Create Outputs for sentence and sample parameters\n",
    "\n",
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        \"Generate a test case for the following UI Element:\", # instruction #\n",
    "        \"Link Element : \", # input\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    ),\n",
    "    alpaca_prompt.format(\n",
    "        \"Generate a test case for the following UI Element:\", # instruction #\n",
    "        \"Link Element : \", # input\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    ),\n",
    "\n",
    "    alpaca_prompt.format(\n",
    "        \"Generate a test case for the following UI Element:\", # instruction\n",
    "        \"Button Element : \", # input\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    ),\n",
    "    alpaca_prompt.format(\n",
    "        \"Generate a test case for the following UI Element:\", # instruction\n",
    "        \"Button Element : \", # input\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    ),\n",
    "\n",
    "    alpaca_prompt.format(\n",
    "        \"Generate a test case for the following UI Element:\", # instruction #\n",
    "        \"Header Element : \", #\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    ),\n",
    "    alpaca_prompt.format(\n",
    "        \"Generate a test case for the following UI Element:\", # instruction #\n",
    "        \"Header Element : \", #\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    ),\n",
    "\n",
    "    alpaca_prompt.format(\n",
    "        \"Generate a test case for the following UI Element:\", # instruction #\n",
    "        \"Input Element : \", #\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    ),\n",
    "    alpaca_prompt.format(\n",
    "        \"Generate a test case for the following UI Element:\", # instruction #\n",
    "        \"Input Element : \", #\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    )\n",
    "\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\n",
    "tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import spacy\n",
    "from selfcheckgpt.modeling_selfcheck import SelfCheckLLMPrompt\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "llm_model = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "selfcheck_prompt = SelfCheckLLMPrompt(llm_model, device)\n",
    "\n",
    "## Manually entered in responses\n",
    "# Response\n",
    "sentences = []\n",
    "# Response 2 / Samples\n",
    "samples = []\n",
    "\n",
    "sent_scores_prompt = selfcheck_prompt.predict(\n",
    "    sentences = sentences,                         \n",
    "    sampled_passages = samples,\n",
    "    verbose = True,\n",
    ")\n",
    "print(sent_scores_prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
