{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Notebook\n",
    "- Evaluating perplexity and hallucination (Automated evaluation metrics)\n",
    "- Perplexity is calculated using Huggingface\n",
    "- Hallucination is calculated using SelfCheckGPT score/s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Installation / Google Drive Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install selfcheckgpt\n",
    "! pip install spacy\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loading LORA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading LORA Model\n",
    "if True:\n",
    "    max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "    dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "    load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "    from unsloth import FastLanguageModel\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = \"/content/drive/MyDrive/Fine-Tuning Llama 3.1 For Test Cases/lora\", # YOUR MODEL YOU USED FOR TRAINING\n",
    "        max_seq_length = max_seq_length,\n",
    "        dtype = dtype,\n",
    "        load_in_4bit = load_in_4bit,\n",
    "    )\n",
    "    FastLanguageModel.for_inference(model) # Enable native 2x faster inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(\"/content/drive/MyDrive/Fine-Tuning Llama 3.1 For Test Cases/cycle2_test_reformatted.csv\")\n",
    "\n",
    "# Combine the columns into a single text input for the model\n",
    "df[\"text\"] = f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{df[\"instruction\"]}\n",
    "\n",
    "### Input:\n",
    "{df[\"input\"]}\n",
    "\n",
    "### Response:\n",
    "{df[\"output\"]}\"\"\"\n",
    "\n",
    "# Create a HuggingFace dataset\n",
    "dataset = Dataset.from_pandas(df[[\"text\"]])\n",
    "\n",
    "# Preprocess the dataset to match the input format\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(preprocess_function, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "# Evaluate Perplexity using Torch\n",
    "def compute_perplexity(model, dataset, tokenizer):\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "    dataloader = DataLoader(tokenized_datasets, collate_fn=data_collator, batch_size=8)\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_length = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader):\n",
    "            inputs = batch[\"input_ids\"].to(device)\n",
    "            outputs = model(inputs, labels=inputs)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item() * inputs.size(1)\n",
    "            total_length += inputs.size(1)\n",
    "\n",
    "    avg_loss = total_loss / total_length\n",
    "    perplexity = torch.exp(torch.tensor(avg_loss))\n",
    "    return perplexity.item()\n",
    "\n",
    "perplexity = compute_perplexity(model, dataset, tokenizer)\n",
    "print(f\"Perplexity: {perplexity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hallucination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import spacy\n",
    "from selfcheckgpt.modeling_selfcheck import SelfCheckLLMPrompt\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "llm_model = \"unsloth/Meta-Llama-3.1-8B\"\n",
    "selfcheck_prompt = SelfCheckLLMPrompt(llm_model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "outputs = selfcheck_prompt.tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        \"Generate a test case for the following UI Element:\", # instruction #\n",
    "        \"Paragraph Element: 'February 6: Sámi National Day (1917); Waitangi Day in New Zealand (1840)' With Color rgb(32, 33, 34) With Background Color rgba(0, 0, 0, 0) from the website: https://en.wikipedia.org/wiki/Main_Page\", # input\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    ),\n",
    "])\n",
    "\n",
    "repeated_outputs = selfcheck_prompt.tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        \"Generate a test case for the following UI Element:\", # instruction #\n",
    "        \"Paragraph Element: 'February 6: Sámi National Day (1917); Waitangi Day in New Zealand (1840)' With Color rgb(32, 33, 34) With Background Color rgba(0, 0, 0, 0) from the website: https://en.wikipedia.org/wiki/Main_Page\", # input\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    ),\n",
    "])\n",
    "\n",
    "sentences = outputs\n",
    "# Response 2 / Samples\n",
    "samples =  repeated_outputs\n",
    "\n",
    "sent_scores_prompt = selfcheck_prompt.predict(\n",
    "    sentences = sentences,\n",
    "    sampled_passages = samples,\n",
    "    verbose = True,\n",
    ")\n",
    "print(sent_scores_prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
