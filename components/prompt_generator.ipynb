{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prototype Code : Prompt Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt Generator + LLM\n",
    "\n",
    "# CONTEXT\n",
    "template = \"\"\"\n",
    "You are a quality assurance expert that generates functional test cases for websites. You take in a UI element and you generate a functional test case.\n",
    "\n",
    "Here is the UI element (some elements have a link attached to them): {question}\n",
    "ONLY output in the following format: \n",
    "\"Objective\"~\"Preconditions\"~\"Test Steps\"~\"Expected Result\"\n",
    "\n",
    "DO NOT output any other text. DO NOT output 'Here are the test cases...', your output should be like the example output below.\n",
    "\n",
    "Example Input:\n",
    "Link Element: Home with URL : https://bicol-u.edu.ph/\n",
    "Link Element: Academics with URL : https://bicol-u.edu.ph/#\n",
    "...\n",
    "\n",
    "Example Output:\n",
    "\"Verify the functionality of the Link Element 'Home'\"~\"The user is on the webpage 'https://bicol-u.edu.ph/'\"~\"'1. User navigates to the webpage \\'https://bicol-u.edu.ph/\\'' '2. Click on Link Element \\'Home\\'' '3. Verify if the webpage opens in a new tab/window.'\"~\"Webpage 'https://bicol-u.edu.ph/' should open in a new tab/window.\"\n",
    "\"Verify the functionality of the Link Element 'Academics'\"~\"The user is on the webpage 'https://bicol-u.edu.ph/'\"~\"'1. User navigates to the webpage \\'https://bicol-u.edu.ph/\\'' '2. Click on Link Element \\'Academics\\'' '3. Verify if the link url changes to \\'https://bicol-u.edu.ph/#\\'' '4. Verify if a dropdown below \\'Academics\\' is visible'\"~\"A dropdown should show below 'Academics', but the webpage does not change\"\n",
    "...\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Load Model Chain\n",
    "def load_model_chain(template : str =  template, model_str : str = \"llama3.1\"):\n",
    "    # Prompt\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "    # Model\n",
    "    model = OllamaLLM(model=model_str)\n",
    "    # Chain\n",
    "    chain = prompt | model\n",
    "    return chain\n",
    "\n",
    "# Create Test Case Data\n",
    "def create_test_cases(data, chain, model_str : str = \"llama3.1\" , template : str = template, batch_size : int = 10):\n",
    "    \n",
    "    return_data = []\n",
    "    \n",
    "    for sub_data in data:\n",
    "        element_test_cases = []\n",
    "        i = 0\n",
    "        j = 0\n",
    "        print(f\"Batch Number: {ceil(len(sub_data)/batch_size)}\")\n",
    "        while (j<ceil(len(sub_data)/batch_size)):\n",
    "            print(f\"[{j}] Batch {str(len(sub_data[i:i+batch_size]))}\")\n",
    "            appending = []\n",
    "            for dat in sub_data[i:i+batch_size]:\n",
    "                appending.append(chain.invoke({\"question\": str(dat)}))\n",
    "            element_test_cases.append(appending)\n",
    "            i+=batch_size\n",
    "            j+=1\n",
    "            prompt = ChatPromptTemplate.from_template(template)\n",
    "            model = OllamaLLM(model=model_str)\n",
    "            chain = prompt | model\n",
    "        return_data.append(element_test_cases)\n",
    "    return return_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version 1 : Dissection and Simplification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Langchain and Ollama\n",
    "import langchain\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# CONTEXT V1\n",
    "template = \"\"\"\n",
    "You are a quality assurance expert that generates functional test cases for websites. You take in a UI element and you generate a functional test case for usability.\n",
    "\n",
    "Some UI elements have a link attached to them and other properties.\n",
    "ONLY output in the following format: \n",
    "\"Objective\"~\"Preconditions\"~\"Test Steps\"~\"Expected Result\"\n",
    "DO NOT output any other text. DO NOT output 'Here are the test cases...', your output should be like the example output below.\n",
    "\n",
    "Example Input:\n",
    "Link Element: Home with URL : https://bicol-u.edu.ph/\n",
    "Link Element: Academics with URL : https://bicol-u.edu.ph/#\n",
    "...\n",
    "\n",
    "Example Output:\n",
    "\"Verify the functionality of the Link Element 'Home'\"~\"The user is on the webpage 'https://bicol-u.edu.ph/'\"~\"'1. User navigates to the webpage \\'https://bicol-u.edu.ph/\\'' '2. Click on Link Element \\'Home\\'' '3. Verify if the webpage opens in a new tab/window.'\"~\"Webpage 'https://bicol-u.edu.ph/' should open in a new tab/window.\"\n",
    "\"Verify the functionality of the Link Element 'Academics'\"~\"The user is on the webpage 'https://bicol-u.edu.ph/'\"~\"'1. User navigates to the webpage \\'https://bicol-u.edu.ph/\\'' '2. Click on Link Element \\'Academics\\'' '3. Verify if the link url changes to \\'https://bicol-u.edu.ph/#\\'' '4. Verify if a dropdown below \\'Academics\\' is visible'\"~\"A dropdown should show below 'Academics', but the webpage does not change\"\n",
    "...\n",
    "\n",
    "Here is the UI element : {question}\n",
    "\"\"\"\n",
    "\n",
    "# Prompt Generator + LLM\n",
    "# TODO : Experiment With Prompts\n",
    "# TODO : Create Altered Version For Fine-Tuned Model Version (aka System Proper)\n",
    "\n",
    "# Load Model Chain\n",
    "def load_model_chain(template : str =  template, model_str : str = \"llama3.1\"):\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "    model = OllamaLLM(model=model_str)\n",
    "    chain = prompt | model\n",
    "    return chain\n",
    "\n",
    "# Create Test Case Data\n",
    "def create_test_cases(data, model_str : str = \"llama3.1\" , template : str = template):\n",
    "    \n",
    "    # Load LLM Chain\n",
    "    chain = load_model_chain(template, model_str)\n",
    "\n",
    "    # Return Data\n",
    "    return_data = []\n",
    "    \n",
    "    for item in data:\n",
    "        return_data.append(chain.invoke({\"question\": str(item)}))\n",
    "        # LLM Reset To Free Up Context\n",
    "        chain = load_model_chain(template, model_str)\n",
    "\n",
    "    return return_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
