{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prototype Code : Prompt Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt Generator + LLM\n",
    "\n",
    "# CONTEXT\n",
    "template = \"\"\"\n",
    "You are a quality assurance expert that generates functional test cases for websites. You take in a UI element and you generate a functional test case.\n",
    "\n",
    "Here is the UI element (some elements have a link attached to them): {question}\n",
    "ONLY output in the following format: \n",
    "\"Objective\"~\"Preconditions\"~\"Test Steps\"~\"Expected Result\"\n",
    "\n",
    "DO NOT output any other text. DO NOT output 'Here are the test cases...', your output should be like the example output below.\n",
    "\n",
    "Example Input:\n",
    "Link Element: Home with URL : https://bicol-u.edu.ph/\n",
    "Link Element: Academics with URL : https://bicol-u.edu.ph/#\n",
    "...\n",
    "\n",
    "Example Output:\n",
    "\"Verify the functionality of the Link Element 'Home'\"~\"The user is on the webpage 'https://bicol-u.edu.ph/'\"~\"'1. User navigates to the webpage \\'https://bicol-u.edu.ph/\\'' '2. Click on Link Element \\'Home\\'' '3. Verify if the webpage opens in a new tab/window.'\"~\"Webpage 'https://bicol-u.edu.ph/' should open in a new tab/window.\"\n",
    "\"Verify the functionality of the Link Element 'Academics'\"~\"The user is on the webpage 'https://bicol-u.edu.ph/'\"~\"'1. User navigates to the webpage \\'https://bicol-u.edu.ph/\\'' '2. Click on Link Element \\'Academics\\'' '3. Verify if the link url changes to \\'https://bicol-u.edu.ph/#\\'' '4. Verify if a dropdown below \\'Academics\\' is visible'\"~\"A dropdown should show below 'Academics', but the webpage does not change\"\n",
    "...\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Load Model Chain\n",
    "def load_model_chain(template : str =  template, model_str : str = \"llama3.1\"):\n",
    "    # Prompt\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "    # Model\n",
    "    model = OllamaLLM(model=model_str)\n",
    "    # Chain\n",
    "    chain = prompt | model\n",
    "    return chain\n",
    "\n",
    "# Create Test Case Data\n",
    "def create_test_cases(data, chain, model_str : str = \"llama3.1\" , template : str = template, batch_size : int = 10):\n",
    "    \n",
    "    return_data = []\n",
    "    \n",
    "    for sub_data in data:\n",
    "        element_test_cases = []\n",
    "        i = 0\n",
    "        j = 0\n",
    "        print(f\"Batch Number: {ceil(len(sub_data)/batch_size)}\")\n",
    "        while (j<ceil(len(sub_data)/batch_size)):\n",
    "            print(f\"[{j}] Batch {str(len(sub_data[i:i+batch_size]))}\")\n",
    "            appending = []\n",
    "            for dat in sub_data[i:i+batch_size]:\n",
    "                appending.append(chain.invoke({\"question\": str(dat)}))\n",
    "            element_test_cases.append(appending)\n",
    "            i+=batch_size\n",
    "            j+=1\n",
    "            prompt = ChatPromptTemplate.from_template(template)\n",
    "            model = OllamaLLM(model=model_str)\n",
    "            chain = prompt | model\n",
    "        return_data.append(element_test_cases)\n",
    "    return return_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
